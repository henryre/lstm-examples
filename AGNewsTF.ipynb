{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from lstm_examples import util\n",
    "\n",
    "np.random.seed(1701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "n_classes = 4\n",
    "text, y = util.read_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle sentences\n",
    "shuffle_idx = np.random.permutation(len(text))\n",
    "text = [text[i] for i in shuffle_idx]\n",
    "y = y[shuffle_idx]\n",
    "\n",
    "# Train and test split (just take a thousand for speed)\n",
    "n_train, n_test = 1000, 500\n",
    "text_train = text[:n_train]\n",
    "text_test = text[n_train : n_train+n_test]\n",
    "y_train = y[:n_train]\n",
    "y_test = y[n_train : n_train+n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFP - How to create sustainable employment for impoverished, mostly illiterate African populations residing primarily in rural areas is the challenge the African Union aims to tackle at a summit opening Wednesday in Burkina Faso.\n",
      "\n",
      "Class:  0\n"
     ]
    }
   ],
   "source": [
    "print(text_train[0])\n",
    "print(\"\\nClass: \", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse all text\n",
    "text_train_parsed = [nlp(s) for s in text_train]\n",
    "text_test_parsed = [nlp(s) for s in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert text to integer symbols\n",
    "symbol_table = util.SymbolTable()\n",
    "\n",
    "def preprocess_text(parsed_text, symbol_table, init=True):\n",
    "    mapper = symbol_table.lookup_add if init else symbol_table.lookup\n",
    "    return [[mapper(w.text.strip().lower()) for s in t.sents for w in s] for t in parsed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbols_train = preprocess_text(text_train_parsed, symbol_table, True)\n",
    "symbols_test = preprocess_text(text_test_parsed, symbol_table, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congress is now poised to hand President Bush an election-year tax cut victory\n",
      "[1541, 21, 264, 1454, 5, 1542, 150, 882, 555, 1543, 3, 683, 1544, 1277, 1209]\n",
      "\n",
      "Class:  2\n"
     ]
    }
   ],
   "source": [
    "print(text_train_parsed[100][:15])\n",
    "print(symbols_train[100][:15])\n",
    "print(\"\\nClass: \", y_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert symbol lists to a matrix X\n",
    "# Text has a fixed maximum length (50 tokens)\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "\n",
    "def symbols_to_matrix(symbol_lists, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "    m = len(symbol_lists)\n",
    "    x_batch_array = np.zeros((m, max_sentence_length)).astype(int)\n",
    "    len_batch = np.zeros(m).astype(int)\n",
    "    for j, x in enumerate(symbol_lists):\n",
    "        t = min(max_sentence_length, len(x))\n",
    "        x_batch_array[j, :t] = x[:t]\n",
    "        len_batch[j]         = t\n",
    "    return x_batch_array, len_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1045, 1046,   77,   79, 1047,  555, 1048,   83,   27, 1049,  202,\n",
       "         1050,   69, 1051,    3, 1052,    3, 1053,  211, 1054,   28, 1055,\n",
       "         1056,    9,   22,   27, 1049,   41,   34,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  22, 1057,   44,   28,  669,    3,   39, 1058, 1059,  186, 1060,\n",
       "          744,   11,   42,   22, 1061,   44,  436,   21,   28, 1062,   41,\n",
       "           34,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]]), array([29, 23]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_matrix(symbols_train[60:62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "X = tf.placeholder(tf.int32, (None, MAX_SENTENCE_LENGTH))\n",
    "X_len = tf.placeholder(tf.int32, (None,))\n",
    "y = tf.placeholder(tf.int32, (None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create embedding table (one row for each word)\n",
    "d = 100\n",
    "\n",
    "# Create zero row for 0 symbol\n",
    "with tf.variable_scope('embeddings'):\n",
    "    zero  = tf.constant(0.0, dtype=tf.float32, shape=(1, d))\n",
    "    embed = tf.Variable(tf.random_normal(\n",
    "        (symbol_table.num_words() + 1, d), stddev=0.1, seed=1701\n",
    "    ))\n",
    "    U = tf.concat([zero, embed], axis=0, name='embedding_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get out rows from U for each word in each sentence\n",
    "# Dimensions are [batch_size x MAX_SENTENCE_LENGTH x embedding_dimenion]\n",
    "#                [   ???     x        50           x       100         ]\n",
    "\n",
    "word_feats = tf.nn.embedding_lookup(U, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build RNN\n",
    "\n",
    "batch_size = tf.shape(X)[0]\n",
    "\n",
    "with tf.variable_scope(\"LSTM\"):\n",
    "    tf.set_random_seed(1701)\n",
    "    # LSTM cell architecture\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(d)\n",
    "    # Set RNN\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_out, _ = tf.nn.dynamic_rnn(\n",
    "        cell,\n",
    "        word_feats,\n",
    "        sequence_length=X_len,\n",
    "        initial_state=initial_state,\n",
    "        time_major=False               \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n",
    "# The LSTM produced new features for us in rnn_out\n",
    "# There is one output per word, and we used the same out dimension as the embedding dimension\n",
    "# Dimensions are [batch_size x MAX_SENTENCE_LENGTH x embedding_dimenion]\n",
    "#                [   ???     x        50           x       100         ]\n",
    "# Let's pull out just the last one to describe the whole sentence\n",
    "\n",
    "def get_rnn_output(output, dim, lengths):\n",
    "    \"\"\"Get last output of RNN\"\"\"\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    index = tf.range(0, batch_size) * max_length + (lengths - 1)\n",
    "    flat = tf.reshape(output, [-1, dim])\n",
    "    h = tf.gather(flat, index)\n",
    "    return h\n",
    "\n",
    "final_feats = get_rnn_output(rnn_out, d, X_len)\n",
    "\n",
    "with tf.variable_scope(\"linear\"):\n",
    "    W = tf.Variable(tf.random_normal((d, n_classes), stddev=0.1, seed=1701))\n",
    "    b = tf.Variable(tf.zeros((n_classes,)))\n",
    "    h = tf.nn.xw_plus_b(final_feats, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Output quantities\n",
    "\n",
    "# Probabilistic predictions\n",
    "predictions = tf.nn.softmax(h)\n",
    "predictions_hard = tf.squeeze(tf.argmax(h, axis=1))\n",
    "\n",
    "# Loss and SGD\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)\n",
    ")\n",
    "train_op = tf.train.AdamOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   \tTrain acc=23.1%\t\tTest acc=22.0%\n",
      "Epoch 1   \tTrain acc=42.5%\t\tTest acc=30.8%\n",
      "Epoch 2   \tTrain acc=60.0%\t\tTest acc=45.2%\n",
      "Epoch 3   \tTrain acc=65.6%\t\tTest acc=50.8%\n",
      "Epoch 4   \tTrain acc=79.9%\t\tTest acc=59.0%\n",
      "Epoch 5   \tTrain acc=95.2%\t\tTest acc=63.2%\n",
      "Epoch 6   \tTrain acc=81.8%\t\tTest acc=48.8%\n",
      "Epoch 7   \tTrain acc=96.6%\t\tTest acc=61.6%\n",
      "Epoch 8   \tTrain acc=96.2%\t\tTest acc=60.6%\n",
      "Epoch 9   \tTrain acc=98.4%\t\tTest acc=64.8%\n",
      "Epoch 10  \tTrain acc=99.1%\t\tTest acc=65.6%\n",
      "Epoch 11  \tTrain acc=99.8%\t\tTest acc=66.0%\n",
      "Epoch 12  \tTrain acc=99.9%\t\tTest acc=65.0%\n",
      "Epoch 13  \tTrain acc=98.7%\t\tTest acc=66.2%\n",
      "Epoch 14  \tTrain acc=90.0%\t\tTest acc=61.6%\n",
      "Epoch 15  \tTrain acc=99.8%\t\tTest acc=66.8%\n",
      "Epoch 16  \tTrain acc=99.8%\t\tTest acc=68.2%\n",
      "Epoch 17  \tTrain acc=99.9%\t\tTest acc=68.2%\n",
      "Epoch 18  \tTrain acc=99.9%\t\tTest acc=67.8%\n",
      "Epoch 19  \tTrain acc=99.9%\t\tTest acc=67.8%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "X_train_all, X_train_len_all = symbols_to_matrix(symbols_train)\n",
    "X_test_all, X_test_len_all = symbols_to_matrix(symbols_test)\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    # Print status\n",
    "    pred_train = session.run(predictions_hard, {X: X_train_all, X_len: X_train_len_all})\n",
    "    train_acc = np.mean(pred_train == y_train)\n",
    "    pred_test = session.run(predictions_hard, {X: X_test_all, X_len: X_test_len_all})\n",
    "    test_acc = np.mean(pred_test == y_test)\n",
    "    print(\"Epoch {0:<4}\\tTrain acc={1:.1f}%\\t\\tTest acc={2:.1f}%\".format(t, 100*train_acc, 100*test_acc))\n",
    "    # Run batches\n",
    "    for i in range(0, len(symbols_train), batch_size):\n",
    "        X_batch, X_len_batch = symbols_to_matrix(symbols_train[i : i+batch_size])\n",
    "        y_batch = y_train[i : i+batch_size]\n",
    "        loss_batch = session.run([loss, train_op], {X: X_batch, X_len: X_len_batch, y: y_batch})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
