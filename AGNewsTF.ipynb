{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from lstm_examples import util\n",
    "\n",
    "np.random.seed(1701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "n_classes = 4\n",
    "text, y = util.read_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle sentences\n",
    "shuffle_idx = np.random.permutation(len(text))\n",
    "text = [text[i] for i in shuffle_idx]\n",
    "y = y[shuffle_idx]\n",
    "\n",
    "# Train and test split (just take a thousand for speed)\n",
    "n_train, n_test = 1000, 500\n",
    "text_train = text[:n_train]\n",
    "text_test = text[n_train : n_train+n_test]\n",
    "y_train = y[:n_train]\n",
    "y_test = y[n_train : n_train+n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_train[0])\n",
    "print(\"\\nClass: \", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse all text\n",
    "text_train_parsed = [nlp(s) for s in text_train]\n",
    "text_test_parsed = [nlp(s) for s in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert text to integer symbols\n",
    "symbol_table = util.SymbolTable()\n",
    "\n",
    "def preprocess_text(parsed_text, symbol_table, init=True):\n",
    "    mapper = symbol_table.lookup_add if init else symbol_table.lookup\n",
    "    return [[mapper(w.text.strip().lower()) for s in t.sents for w in s] for t in parsed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_train = preprocess_text(text_train_parsed, symbol_table, True)\n",
    "symbols_test = preprocess_text(text_test_parsed, symbol_table, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_train_parsed[100][:15])\n",
    "print(symbols_train[100][:15])\n",
    "print(\"\\nClass: \", y_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert symbol lists to a matrix X\n",
    "# Text has a fixed maximum length (50 tokens)\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "\n",
    "def symbols_to_matrix(symbol_lists, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "    m = len(symbol_lists)\n",
    "    x_batch_array = np.zeros((m, max_sentence_length)).astype(int)\n",
    "    len_batch = np.zeros(m).astype(int)\n",
    "    for j, x in enumerate(symbol_lists):\n",
    "        t = min(max_sentence_length, len(x))\n",
    "        x_batch_array[j, :t] = x[:t]\n",
    "        len_batch[j]         = t\n",
    "    return x_batch_array, len_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_matrix(symbols_train[60:62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "X = tf.placeholder(tf.int32, (None, MAX_SENTENCE_LENGTH))\n",
    "X_len = tf.placeholder(tf.int32, (None,))\n",
    "y = tf.placeholder(tf.int32, (None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create embedding table (one row for each word)\n",
    "d = 100\n",
    "\n",
    "# Create zero row for 0 symbol\n",
    "with tf.variable_scope('embeddings'):\n",
    "    zero  = tf.constant(0.0, dtype=tf.float32, shape=(1, d))\n",
    "    embed = tf.Variable(tf.random_normal(\n",
    "        (symbol_table.num_words() + 1, d), stddev=0.1, seed=1701\n",
    "    ))\n",
    "    U = tf.concat([zero, embed], axis=0, name='embedding_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out rows from U for each word in each sentence\n",
    "# Dimensions are [batch_size x MAX_SENTENCE_LENGTH x embedding_dimenion]\n",
    "#                [   ???     x        50           x       100         ]\n",
    "\n",
    "word_feats = tf.nn.embedding_lookup(U, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RNN\n",
    "\n",
    "batch_size = tf.shape(X)[0]\n",
    "\n",
    "with tf.variable_scope(\"LSTM\") as scope:\n",
    "    tf.set_random_seed(1701)\n",
    "    # LSTM cell architecture\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(d)\n",
    "    # Set RNN\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_out, _ = tf.nn.dynamic_rnn(\n",
    "        cell,\n",
    "        word_feats,\n",
    "        sequence_length=X_len,\n",
    "        initial_state=initial_state,\n",
    "        time_major=False               \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n",
    "# The LSTM produced new features for us in rnn_out\n",
    "# There is one output per word, and we used the same out dimension as the embedding dimension\n",
    "# Dimensions are [batch_size x MAX_SENTENCE_LENGTH x embedding_dimenion]\n",
    "#                [   ???     x        50           x       100         ]\n",
    "# Let's pull out just the last one to describe the whole sentence\n",
    "\n",
    "def get_rnn_output(output, dim, lengths):\n",
    "    \"\"\"Get last output of RNN\"\"\"\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    index = tf.range(0, batch_size) * max_length + (lengths - 1)\n",
    "    flat = tf.reshape(output, [-1, dim])\n",
    "    h = tf.gather(flat, index)\n",
    "    return h\n",
    "\n",
    "final_feats = get_rnn_output(rnn_out, d, X_len)\n",
    "\n",
    "with tf.variable_scope(\"linear\"):\n",
    "    W = tf.Variable(tf.random_normal((d, n_classes), stddev=0.1, seed=1701))\n",
    "    b = tf.Variable(tf.zeros((n_classes,)))\n",
    "    h = tf.nn.xw_plus_b(final_feats, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output quantities\n",
    "\n",
    "# Probabilistic predictions\n",
    "predictions = tf.nn.softmax(h)\n",
    "predictions_hard = tf.squeeze(tf.argmax(h, axis=1))\n",
    "\n",
    "# Loss and SGD\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)\n",
    ")\n",
    "train_op = tf.train.AdamOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "X_train_all, X_train_len_all = symbols_to_matrix(symbols_train)\n",
    "X_test_all, X_test_len_all = symbols_to_matrix(symbols_test)\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    # Print status\n",
    "    pred_train = session.run(predictions_hard, {X: X_train_all, X_len: X_train_len_all})\n",
    "    train_acc = np.mean(pred_train == y_train)\n",
    "    pred_test = session.run(predictions_hard, {X: X_test_all, X_len: X_test_len_all})\n",
    "    test_acc = np.mean(pred_test == y_test)\n",
    "    print(\"Epoch {0:<4}\\tTrain acc={1:.1f}%\\t\\tTest acc={2:.1f}%\".format(t, 100*train_acc, 100*test_acc))\n",
    "    # Run batches\n",
    "    for i in range(0, len(symbols_train), batch_size):\n",
    "        X_batch, X_len_batch = symbols_to_matrix(symbols_train[i : i+batch_size])\n",
    "        y_batch = y_train[i : i+batch_size]\n",
    "        loss_batch = session.run([loss, train_op], {X: X_batch, X_len: X_len_batch, y: y_batch})[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
